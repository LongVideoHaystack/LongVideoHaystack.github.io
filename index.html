<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LV-Haystack</title>
    <style>
        
body {
    font-family: Arial, sans-serif;
    background: #ffffff;
    font-size: 14px;
    margin: 0;
    padding: 0;
}
        .infographic-container {
          display: flex;
          flex-wrap: wrap;
          gap: 16px;
          justify-content: center;
          align-items: flex-start;
        }
        .info-card {
          background: #fff;
          border-radius: 10px;
          box-shadow: 0 2px 6px rgba(0,0,0,0.1);
          width: 300px;
          padding: 20px;
          text-align: left;
        }
        .info-card img {
          display: block;       /* 变成块级元素以便使用 margin:0 auto */
          margin: 0 auto 8px;   /* 水平居中 + 设置8px底部外边距 */
          width: 40px;
          height: 40px;
          object-fit: contain;
        }
        .info-title {
          margin: 8px 0 4px;
          font-size: 16px; /* 原先 18px，稍降 */
          font-weight: bold;
          text-align: center;
        }
        .info-subtitle {
          color: #777;
          margin-bottom: 10px;
          font-size: 14px;
          text-align: center;
        }
        .info-card p {
          line-height: 0.5;   
          /* 字体颜色（示例 #777 比 #000 浅一些）*/
          color: #777;        
          margin: 4px 0;      
        }

        #abstract h1, #abstract p {
            color: #000000;
            text-align: justify;
            text-justify: inter-word; 
        }
        h1 {
            color: #4E2A84;
        }
        h2 {
            color: #4E2A84;
            text-align: justify;
            font-size: medium;
        }
        h3 {
            text-align: justify;
            font-size: medium;
        }
        ul {
            list-style-type: disc;
            margin-left: 20px;
        }
        li {
            margin-bottom: 10px;
        }
        .summary {
            margin-top: 20px;
            font-weight: bold;
        }
        .author-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 10px;
            justify-content: center;
            align-items: center;
            margin-top: 10px;
        }

        .author-name a {
            text-decoration: none;
            color: #000000;
            font-size: 14px;
        }

        .author-name a:hover {
            text-decoration: underline;
        }
        .slideshow-container {
          position: relative;
          max-width: 100%;
          margin: auto;
          overflow: hidden; /* 若有视频溢出，隐藏 */
        }

        /* 每个幻灯片 */
        .mySlides {
          display: flex;
          gap: 20px;
          justify-content: center;
          align-items: flex-start; /* 如果高度有差异，可让顶部对齐 */
        }

        .mySlides > div {
          flex: 1;               /* 两列平均分 */
          max-width: 600px;      /* 可以给个最大宽度限制 */
          box-sizing: border-box;
        }

        .mySlides video {
          width: 100%;
          height: auto;
          vertical-align: middle; /* 防止底部出现几像素的空隙 */
        }

        /* 幻灯片内文字 */
        .text {
          /* 如果之前有下述属性，可暂时注释掉或移除 */
          /* white-space: nowrap;
          text-overflow: ellipsis;
          overflow: hidden; */
          white-space: normal;   
          overflow: visible;  
        }

        /* 上/下一个 按钮 */
        .prev, .next {
          cursor: pointer;
          position: absolute;
          top: 50%;
          width: auto;
          padding: 16px;
          margin-top: -22px;
          color: white;
          font-weight: bold;
          font-size: 18px;
          border-radius: 0 3px 3px 0;
          user-select: none;
          z-index: 1;
          background-color: rgba(0, 0, 0, 0.5); /* 半透明背景 */
        }
        .next {
          right: 0;
          border-radius: 3px 0 0 3px;
        }
        .prev:hover, .next:hover {
          background-color: rgba(0, 0, 0, 0.8);
        }

        /* 小圆点指示器样式 */
        .dot {
          cursor: pointer;
          height: 15px; 
          width: 15px; 
          margin: 0 2px; 
          background-color: #bbb; 
          border-radius: 50%;
          display: inline-block; 
          transition: background-color 0.6s ease;
        }
        .active, .dot:hover {
          background-color: #717171;
          p {
            text-align: justify; /* 两边对齐 */
          }
          .full-page-image {
            position: relative;
            width: 100%;
            min-height: 100px; 
          /* 如果想保持窗口全屏高度，可用 height: 100vh; */
          /* height: 100vh; */
            overflow: hidden;  /* 防止子元素溢出 */
          }

          .full-page-image img {
            display: block;         /* 避免默认内联元素空隙 */
            width: 100%;            /* 让图片随父容器宽度自适应 */
            height: auto;           /* 等比例缩放，不至于溢出 */
            object-fit: cover;      /* 如果想让图片完全覆盖容器，但可能截取部分 */
          }

          .overlay {
            position: absolute; 
            top: 0; left: 0; 
            right: 0; bottom: 0;
            background: rgba(0, 0, 0, 0.3); /* 半透明蒙层示例 */
          }

          .content {
            position: relative; /* 确保在 overlay 之上 */
            z-index: 1;         
            color: #fff;        /* 文字颜色看需求 */
          /* 其他排版、间距等 */
          }
        }

        /* ---------------------------
           NEW STYLES FOR OBJECT-SIZE
           SECTION (QUESTION / ANSWER)
        ---------------------------- */
        .object-size-container {
          max-width: 600px;
          margin: 30px auto;
          background: #fff;
          padding: 20px;
          border-radius: 8px;
          box-shadow: 0 2px 6px rgba(0,0,0,0.1);
        }

        .object-size-container h2 {
          margin-top: 0;
        }
        .label {
            position: absolute;
            top: 10px;
            left: 10px;
            background-color: #4E2A84; /* or your choice of purple */
            color: #fff;
            padding: 8px 12px;
            border-radius: 4px;
            font-weight: bold;
            text-transform: uppercase;
            z-index: 10; /* Ensures it stays on top */
        }

        .object-video-container {
          margin: 20px 0;
          text-align: center;
          position: relative; 
          display: inline-block;
        }

        .object-video-container video {
          max-width: 100%;
          height: auto;
          border: 1px solid #ccc;
          border-radius: 8px;
          position: relative; 
          display: inline-block;
        }

        .object-question {
          font-weight: bold;
          margin: 20px 0 10px;
        }

        .reveal-button {
          background-color: #4E2A84;
          color: #fff;
          padding: 10px 15px;
          border: none;
          cursor: pointer;
          border-radius: 4px;
          margin-top: 10px;
        }

        /* .reveal-button:hover {
          background-color: #0056b3;
        } */

        .answer {
          display: none;
          /* margin-top: 15px; */
          /* padding: 15px; */
          /* border: 1px solid #ddd; */
          /* border-radius: 4px; */
          background-color: #f9f9f9;
        }

        /* ---------------------------------- */
        /* NEW STYLES FOR DATASET VIEWER      */
        /* ---------------------------------- */
        .dataset-viewer {
            max-width: 800px;
            margin: 40px auto;
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 10px rgba(0,0,0,0.1);
        }
        
        .dataset-title {
            color: #4E2A84;
            text-align: center;
            margin-bottom: 20px;
            font-size: 24px;
        }
        
        .video-container {
            position: relative;
            width: 100%;
            margin: 20px 0;
            background-color: #000;
            overflow: hidden;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        
        .video-placeholder {
            width: 100%;
            aspect-ratio: 16/9;
            display: flex;
            justify-content: center;
            align-items: center;
            background-color: #333;
            color: white;
            font-size: 16px;
            position: relative;
        }
        
        .video-controls {
            display: flex;
            justify-content: space-between;
            align-items: center;
            background-color: rgba(0,0,0,0.7);
            padding: 10px;
            color: white;
        }
        
        .play-button {
            background: none;
            border: none;
            color: white;
            font-size: 24px;
            cursor: pointer;
        }
        
        .timeline {
            flex-grow: 1;
            margin: 0 15px;
            height: 5px;
            background-color: #555;
            position: relative;
            border-radius: 5px;
            cursor: pointer;
        }
        
        .timeline-progress {
            position: absolute;
            height: 100%;
            background-color: #4E2A84;
            border-radius: 5px;
            width: 50%;
        }
        
        .time-display {
            font-size: 14px;
            margin-left: 10px;
        }
        
        .fullscreen-button {
            background: none;
            border: none;
            color: white;
            font-size: 18px;
            cursor: pointer;
            margin-left: 10px;
        }
        
        .question-section {
            margin: 20px 0;
            padding: 20px;
            background-color: white;
            border-radius: 10px;
            box-shadow: 0 2px 6px rgba(0,0,0,0.1);
        }
        
        .question-title {
            font-size: 18px;
            font-weight: bold;
            color: #4E2A84;
            margin-bottom: 15px;
        }
        
        .question-text {
            font-size: 16px;
            margin-bottom: 20px;
        }
        
        .options-container {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 10px;
            margin-bottom: 20px;
        }
        
        .option {
            display: flex;
            flex-direction: column;
            align-items: center;
            text-align: center;
        }
        
        .option-radio {
            margin-bottom: 5px;
        }
        
        .option-label {
            font-weight: bold;
        }
        
        .dataset-reveal-button {
            background-color: #4E2A84;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            font-weight: bold;
            display: flex;
            align-items: center;
            margin: 20px auto;
        }
        
        .dataset-reveal-button img {
            width: 24px;
            height: 24px;
            margin-right: 10px;
        }
        
        .answer-container {
            display: none;
            margin-top: 20px;
            padding: 20px;
            background-color: #f0f0f0;
            border-radius: 10px;
            border-left: 5px solid #4E2A84;
        }
        
        .keyframe-section {
            margin-top: 15px;
        }
        
        .keyframe-title {
            font-weight: bold;
            color: #4E2A84;
            margin-bottom: 10px;
        }
        
        .keyframe-image {
            width: 100%;
            max-width: 600px;
            border-radius: 8px;
            border: 1px solid #ddd;
            margin: 0 auto;
            display: block;
        }
        
        .answer-section {
            margin-top: 20px;
        }
        
        .answer-title {
            font-weight: bold;
            color: #4E2A84;
            margin-bottom: 10px;
        }
        
        .ground-truth, .mllm-answer {
            padding: 10px;
            background-color: white;
            border-radius: 5px;
            margin-bottom: 10px;
        }
        
        .dataset-navigation {
            display: flex;
            justify-content: space-between;
            margin: 20px 0;
        }
        
        .nav-button {
            background-color: #4E2A84;
            color: white;
            border: none;
            padding: 10px 15px;
            border-radius: 5px;
            cursor: pointer;
        }
        
        .nav-button:disabled {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        
        .dataset-label {
            position: absolute;
            top: 10px;
            left: 10px;
            background-color: #4E2A84;
            color: white;
            padding: 5px 10px;
            border-radius: 5px;
            font-size: 12px;
            font-weight: bold;
            z-index: 10;
        }
    </style>
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="apple-touch-icon" sizes="180x180" href="assets/img/icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/icon.png">
    <link rel="icon" type="image/png" sizes="16x16" href="assets/img/icon.png">
    <link rel="manifest" href="/site.webmanifest">

    <meta property="og:type" content="website"/>
    <meta property="og:image" content="assets/img/icon.png"/>
    <meta property="og:image:type" content="image/png">
    <meta property="og:url" content="https://lv-haystack.github.io/"/>
    <meta property="og:title" content="LV-Haystack: Re-thinking Temporal Search for Long-Form Video Understanding"/>
    <meta property="og:description" content="Introducing T* and LV-Haystack, the latest advancements in Vision-Language Models for Comprehensive Long Video Understanding"/>

    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:title" content="LV-Haystack: Temporal Search for Long-Form Video Understanding"/>
    <meta name="twitter:description" content="Introducing T* and LV-Haystack for efficient long-form video understanding"/>
    <meta name="twitter:creator" content="@lvhaystack"/>

    <meta name="twitter:label1" content="Published at"/>
    <meta name="twitter:data1" content="February 2025"/>
    <meta name="twitter:label2" content="Reading time"/>
    <meta name="twitter:data2" content="5 minutes"/>
</head>
<body>

<div style="max-width: 90%; margin: 20px auto; background-color: white; border-radius: 10px; overflow: hidden; box-shadow: 0 4px 8px rgba(0,0,0,0.1); position: relative;">
    <video
      id="mainVideo"
      src="assets/videos/main.mp4"
      autoplay
      loop
      muted
      playsinline
      style="width: 100%; height: auto; display: block; object-fit: contain; max-height: 80vh; background-color: white;"
    ></video>
    <div id="videoTitle" class="content" style="padding: 20px; position: absolute; bottom: 0; left: 0; right: 0; z-index: 2; background-color: rgba(255,255,255,0.8); opacity: 1; transition: opacity 2s;">
        <h1 style="margin-bottom: 8px; color: #4E2A84;">Needle-in-Haystack in LongVideo Understanding</h1>
        <p style="color: #555;"> —— Re-thinking Temporal Search for Long-Form Video Understanding</p>
    </div>
</div>

<script>
  // Make the title fade out after a few seconds
  document.addEventListener('DOMContentLoaded', function() {
    const videoTitle = document.getElementById('videoTitle');
    const mainVideo = document.getElementById('mainVideo');
    
    // Start the fade out after video has played for 5 seconds
    setTimeout(function() {
      videoTitle.style.opacity = '0';
    }, 5000);
    
    // When video is reset (looped), show the title again
    mainVideo.addEventListener('seeked', function() {
      // If the video is close to the beginning (within 0.5 seconds)
      if (mainVideo.currentTime < 0.5) {
        videoTitle.style.opacity = '1';
        
        // And fade it out again after 5 seconds
        setTimeout(function() {
          videoTitle.style.opacity = '0';
        }, 5000);
      }
    });
  });
</script>

<div id="title_slide">
    <div class="title_left">
        
        <h1 style="color:#4E2A84;"> Re-thinking Temporal Search for Long-Form Video Understanding </h1>
        <div class="author-container">
            <div class="author-name"><a href="https://jhuiye.com/" target="_blank">Jinhui Ye<sup>1</sup></a></div>
            <div class="author-name"><a href="https://zihanwang314.github.io/" target="_blank">Zihan Wang<sup>2</sup></a></div>
            <div class="author-name"><a href="https://haosensun.github.io/" target="_blank">Haosen Sun<sup>2</sup></a></div>
            <div class="author-name"><a href="https://keshik6.github.io/" target="_blank">Keshigeyan Chandrasegaran<sup>1</sup></a></div>
            <div class="author-name"><a href="https://zanedurante.github.io/" target="_blank">Zane Durante<sup>1</sup></a></div>
            <div class="author-name"><a href="https://ceyzaguirre4.github.io/" target="_blank">Cristobal Eyzaguirre<sup>1</sup></a></div>
            <div class="author-name"><a href="https://talkingtorobots.com/yonatanbisk.html" target="_blank">Yonatan Bisk<sup>3</sup></a></div>
            <div class="author-name"><a href="https://www.niebles.net/" target="_blank">Juan Carlos Niebles<sup>1</sup></a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/ehsan-adeli" target="_blank">Ehsan Adeli<sup>1</sup></a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei<sup>1</sup></a></div>
            <div class="author-name"><a href="https://jiajunwu.com/" target="_blank">Jiajun Wu<sup>1</sup></a></div>
            <div class="author-name"><a href="https://limanling.github.io/" target="_blank">Manling Li<sup>2</sup></a></div>
        </div>
        <br>
        <p class="affiliation">
            <sup>1</sup>Stanford University &nbsp; 
            <sup>2</sup>Northwestern University &nbsp; 
            <sup>3</sup>Carnegie Mellon University &nbsp; 
        </p>
        <br>
        <div class="button-container">
            <a href="#" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <a href="assets/pdf/5448_Re_thinking_Temporal_Sear.pdf" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
            <a href="#" target="_blank" class="button"><i
                    class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a>
            <a href="https://github.com/LongVideoHaystack/TStar" target="_blank" class="button"><i
                    class="fa-light fa-code"></i>&emsp14;Code</a>
            <a href="http://mll-4090-3.cs.northwestern.edu:8082/" target="_blank" class="button"><i
                    class="fa-light fa-robot-astromech"></i>&emsp14;Demo</a>
            <a href="https://huggingface.co/datasets/LVHaystack/LongVideoHaystack" target="_blank" class="button"><i
                    class="fa-light fa-face-smiling-hands"></i>&emsp14;Data</a>
        </div>
        
        <div id="abstract">
            <h2> We Introduce <strong><em>T*</em></strong> and <strong>LV-Haystack</strong>, the latest advancements in Vision-Language Models (VLMs) for Comprehensive Long Video Understanding. 
            </h2>
            <br>
            <h3>&#x1F31F; Our Contributions:</h3>
            <br>
            <ul>
                <li><strong>Lightweight Plugin:</strong> T* enhances LLaVA-OV-72B from 56% to 62% and GPT-4o from 50% to 53% using just 32 frames.</li>
                <li><strong>Faster Inference:</strong> Reduced latency from 34.9 seconds to 10.4 seconds and lowered computational demand from 691 TFLOPs to 170 TFLOPs compared to the previous state-of-the-art.</li>
                <li><strong>Large-Scale Dataset:</strong> LV-Haystack includes 300 hours of video content and 11,000 samples.</li>
            </ul>
        </div>

        <div class="slideshow-container">
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/Demo_0122_c1q2.mp4" type="video/mp4">
                </video>
                <div class="text">Q1: Who did I talk to in the living room?</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/Demo_0122_v1.mp4" type="video/mp4">
                </video>
                <div class="text">Q2: Where was the white trash can before I raised it?</div>
            </div>
            <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
            <a class="next" onclick="plusSlides(1)">&#10095;</a>
        </div>
        <div style="text-align:center">
            <span class="dot" onclick="currentSlide(1)"></span>
            <span class="dot" onclick="currentSlide(2)"></span>
        </div>
        

        <div id="overview">
            <h1>Abstract</h1>
            <p>
                We introduce the "Long Video Haystack" task: finding a minimal set of relevant frames (one to five) from tens of thousands of frames for given queries. We provide LV-HAYSTACK, a new benchmark of 3,874 human-annotated instances with fine-grained metrics for keyframe search quality and efficiency. Current methods only achieve a 2.1% temporal F1 score on its LVBENCH subset, highlighting a large gap.
            <p>
                To address this, we propose <em>T*</em>, a lightweight keyframe search framework that reframes expensive temporal search as a spatial search. <em>T*</em> leverages image-based visual localization capabilities and introduces adaptive zooming-in across temporal and spatial dimensions. Under a 32-frame inference budget, <em>T*</em> boosts GPT-4o's performance from 50.5% to <strong>53.1%</strong>, and LLaVA-OneVision-OV-72B's from 55.5% to <strong>62.4%</strong> on the LongVideoBench XL subset.              
            <p>
        </div>

    </div>
</div>
<hr class="rounded">

<div></div>


<div style="text-align: center; margin-top: 20px;">
    <img 
      src="assets/img/image.png" 
      alt="Sample Image" 
      width="600px" 
      style="display: block; margin: 0 auto;"
    >
    <div style="
         width: 600px; 
         margin: 5px auto 0 auto; 
         text-align: left; 
         color: #555;
    ">
      <p style="font-size: 10px; text-align: justify;">
        <strong>Figure 1.</strong> Extrinsic evaluation results demonstrate how <em>T*</em> improves VLMs by selecting 8 keyframes (needle) from a large haystack, highlighting the significance of <strong>vision-centric search</strong>.
      </p>
    </div>
  </div>
  
<div id="overview">
    <h1>Method Overview</h1>
    <div style="font-family: Arial, sans-serif; text-align: justify;">
        <p>
            <strong> <em>T*</em></strong> is an advanced temporal search framework designed to efficiently identify key frames relevant to specific queries.
            By transforming <strong>temporal search</strong> into <strong>spatial search</strong>, <em>T*</em> leverages lightweight object detectors and Visual Language Model (VLM) visual grounding techniques to streamline the process.
            <em>T*</em> demonstrates exceptional performance, both with and without additional training, making it a versatile and powerful tool for various applications.
        </p>
    </div>

    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/methodv10.png" alt="Diagram illustrating the methodology of the project" width="600px" height="250px">
        <div style="text-align: left; color: #555;">
            <p style="font-size: 10px;"><strong>Figure 2.</strong> <em>T*</em> efficiently searches keyframes in long videos via adaptive temporal and spatial upsampling. First, a VLM identifies target cues from the question (Visual Grounding). Next, a Spatial Searching Model (e.g., YOLO-world) zooms in from coarse to fine frame distributions, avoiding exhaustive frame-by-frame scans. Finally, T* selects K keyframes for QA, serving as visual input to downstream VLMs.</p>
        </div>
    </div>
    
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/algorithm.png" alt="Sample Image" width="600px" height="200px">
    </div>

    <h1>Experiments</h1>
    <p><strong>Evaluations on Downstream Tasks: Video QA<strong></p>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/figure4.png" alt="Sample Image" width="600px" height="230px">
        <div style="text-align: justify; color: #555;">
            <p style="font-size: 10px;"><strong>Table 1.</strong>  QA accuracy (%) of <em>T*</em> used as a frame selection module for VLMs on two long-form QA benchmarks: LongVideoBench and Video-MME (without subtitles for fairness). Top-performing models (in gray) often use many more frames, making comparisons less direct. Models are ranked by their XLong video performance on LongVideoBench and total score on Video-MME, with frame counts noted. All baseline numbers come from their respective publications.</p>
        </div>
    </div>

    <!-- LV-HAYSTACK Data Section -->
    <div id="lv-haystack-data">
        <h1>LV-HAYSTACK Data</h1>
        
        <div class="infographic-container" style="display: flex; flex-wrap: wrap; gap: 16px; justify-content: center; margin: 30px auto;">
            
            <!-- Card 1: Haystack-Ego4D -->
            <div class="info-card" style="background: #fff; border-radius: 10px; box-shadow: 0 2px 6px rgba(0,0,0,0.1); width: 300px; padding: 20px; text-align: left;">
                <img src="assets/logos/DALL1.png" alt="Ego Icon" style="display: block; margin: 0 auto 8px; width: 40px; height: 40px; object-fit: contain;">
                <div class="info-title" style="margin: 8px 0 4px; font-size: 16px; font-weight: bold; text-align: center;">HAYSTACK-EGO4D</div>
                <div class="info-subtitle" style="color: #777; margin-bottom: 10px; font-size: 14px; text-align: center;">Egocentric</div>
                
                <p style="margin: 4px 0; color: #555;"><strong>video</strong>: 244</p>
                <p style="margin: 4px 0; color: #555; white-space: nowrap;"><strong>length</strong>: 101.1 h (~24.8 min/video)</p>
                <p style="margin: 4px 0; color: #555; white-space: nowrap;"><strong>frame</strong>: 10,910,850 (~44,717/video)</p>
                <p style="margin: 4px 0; color: #555; white-space: nowrap;"><strong>QA pair</strong>: 3,874 (~15.9/video)</p>
                <p style="margin: 4px 0; color: #555; white-space: nowrap;"><strong>keyframe</strong>: 18,370 (~4.7/question)</p>
            </div>

            <!-- Card 2: Haystack-LVBench -->
            <div class="info-card" style="background: #fff; border-radius: 10px; box-shadow: 0 2px 6px rgba(0,0,0,0.1); width: 300px; padding: 20px; text-align: left;">
                <img src="assets/logos/DALL2.png" alt="Allocentric Icon" style="display: block; margin: 0 auto 8px; width: 40px; height: 40px; object-fit: contain;">
                <div class="info-title" style="margin: 8px 0 4px; font-size: 16px; font-weight: bold; text-align: center;">HAYSTACK-LVBENCH</div>
                <div class="info-subtitle" style="color: #777; margin-bottom: 10px; font-size: 14px; text-align: center;">Allocentric</div>
                
                <p style="margin: 4px 0; color: #555;"><strong>video</strong>: 246</p>
                <p style="margin: 4px 0; color: #555; white-space: nowrap;"><strong>length</strong>: 57.7 h (~14.1 min/video)</p>
                <p style="margin: 4px 0; color: #555; white-space: nowrap;"><strong>frame</strong>: 4,701,551 (~19,112/video)</p>
                <p style="margin: 4px 0; color: #555; white-space: nowrap;"><strong>QA pair</strong>: 602 (~2.4/video)</p>
                <p style="margin: 4px 0; color: #555; white-space: nowrap;"><strong>keyframe</strong>: 1,071 (~1.8/question)</p>
            </div>
            
        </div>
    </div>
    <h1>Evaluations on Search Utility and Efficiency</h1>
    <p>We explore disentangled evaluation of temporal search & video understanding with 6 fine-grained search metrics.
    </p>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/table2.png" alt="Sample Image" width="600px" height="180px">
        <div style="text-align: justify; color: #555;">
            <p style="font-size: 10px;"><strong>Table 2.</strong> Search efficiency results on LV-HAYSTACK, covering both searching and overall video-understanding performance. Unless noted (e.g., Uniform-32), 8 frames are used for LLaVA-OneVision-72B QA. We report the model plus average call turns for grounding and matching (e.g., VideoAgent calls GPT-4 four times). <em>T*</em> achieves strong QA with minimal search cost, demonstrating efficient long-form video understanding. <sup>†</sup>FLOPs for VideoAgent exclude GPT-4 costs, due to its closed-source nature.</p>
        </div>
    </div>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/table3.png" alt="Sample Image" width="600px" height="220px">
        <div style="text-align: justify; color: #555;">
            <p style="font-size: 10px;"><strong>Table 3.</strong> Searching utility on LV-HAYSTACK. The best 8-frame results are 
                <u>underlined</u>, while the best 32-frame results are <strong>bold</strong>. Increasing 
                searched frames improves recall but reduces precision (e.g., in retrieval methods). 
                Detector-based <em>T*</em> leads the 32-frame setting across multiple metrics, demonstrating 
                visual grounding plus iterative temporal searching. Although attention-based <em>T*</em> 
                performs well at 8 frames, it relies on larger foundation models, impacting efficiency.</p>
        </div>
    </div>

    <!-- ADDING THE INTERACTIVE DATASET VIEWER SECTION HERE -->
    <h1>Dataset Examples</h1>
    <div style="margin: 0 auto;">
        <p>Explore examples from our LV-Haystack dataset and see how our method identifies the needle in the haystack.</p>
    </div>

    <div class="dataset-viewer" style="max-width: 600px; margin: 40px auto;">
        <div class="dataset-navigation">
            <button id="prevExample" class="nav-button" disabled>&larr; Previous Example</button>
            <span id="exampleCounter">Example 1 of 11</span>
            <button id="nextExample" class="nav-button">Next Example &rarr;</button>
        </div>
        
        <div class="video-container">
            <div class="dataset-label" id="datasetType">HAYSTACK-EGO4D</div>
            <div class="video-placeholder" id="videoPlaceholder">
                <video id="previewVideo" src="assets/videos/example1.mp4" controls style="width:100%; background: #333;" poster="assets/img/video_preview1.jpg">
                    Your browser does not support the video tag.
                </video>
            </div>
        </div>
        
        <div class="question-section">
            <div class="question-title">Question:</div>
            <div class="question-text" id="currentQuestion">Measuring from the closest point of each object, which of these objects (table, stool, sofa, stove) is the closest to the TV?</div>
            
            <div class="options-container" id="optionsContainer">
                <div class="option">
                    <input type="radio" id="optionA" name="answer" class="option-radio">
                    <label for="optionA" class="option-label">A. Table</label>
                </div>
                <div class="option">
                    <input type="radio" id="optionB" name="answer" class="option-radio">
                    <label for="optionB" class="option-label">B. Stool</label>
                </div>
                <div class="option">
                    <input type="radio" id="optionC" name="answer" class="option-radio" checked>
                    <label for="optionC" class="option-label">C. Sofa</label>
                </div>
                <div class="option">
                    <input type="radio" id="optionD" name="answer" class="option-radio">
                    <label for="optionD" class="option-label">D. Stove</label>
                </div>
            </div>
            
            <button id="revealButton" class="dataset-reveal-button">
                <img src="assets/img/icon_1.png" alt="icon" width="24" height="24">
                Click to view Human-anotated Keyframes and Ground-truth Answer!
            </button>
            
            <div id="answerContainer" class="answer-container">
                <div class="keyframe-section">
                    <div class="keyframe-title">Keyframe (Needle in Haystack):</div>
                    
                    <!-- Keyframe Slider Container -->
                    <div class="keyframe-slider-container" style="position: relative; margin: 20px 0;">
                        <div class="keyframe-slider" style="display: flex; overflow-x: hidden; scroll-behavior: smooth; border-radius: 8px; background: #f0f0f0; position: relative;">
                            <img class="keyframe-slide" src="assets/img/keyframe1.jpg" alt="Key Frame 1" style="min-width: 100%; height: auto; object-fit: contain;">
                            <img class="keyframe-slide" src="assets/img/keyframe2.jpg" alt="Key Frame 2" style="min-width: 100%; height: auto; object-fit: contain;">
                            <img class="keyframe-slide" src="assets/img/keyframe3.jpg" alt="Key Frame 3" style="min-width: 100%; height: auto; object-fit: contain;">
                        </div>
                        <!-- Slider Navigation Arrows -->
                        <button class="keyframe-prev" style="position: absolute; left: 10px; top: 50%; transform: translateY(-50%); background: rgba(78, 42, 132, 0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; font-size: 18px; cursor: pointer; z-index: 2;">&lt;</button>
                        <button class="keyframe-next" style="position: absolute; right: 10px; top: 50%; transform: translateY(-50%); background: rgba(78, 42, 132, 0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; font-size: 18px; cursor: pointer; z-index: 2;">&gt;</button>
                        
                        <!-- Dots Indicator -->
                        <div class="keyframe-dots" style="display: flex; justify-content: center; margin-top: 10px;">
                            <span class="keyframe-dot active" style="height: 10px; width: 10px; background-color: #bbb; border-radius: 50%; display: inline-block; margin: 0 5px; cursor: pointer;"></span>
                            <span class="keyframe-dot" style="height: 10px; width: 10px; background-color: #bbb; border-radius: 50%; display: inline-block; margin: 0 5px; cursor: pointer;"></span>
                            <span class="keyframe-dot" style="height: 10px; width: 10px; background-color: #bbb; border-radius: 50%; display: inline-block; margin: 0 5px; cursor: pointer;"></span>
                        </div>
                    </div>
                </div>
                
                <div class="answer-section">
                    <div class="answer-title">Ground Truth Option:</div>
                    <div class="ground-truth" id="groundTruth" style="background-color: white; padding: 15px; border-radius: 8px; margin-bottom: 20px;">
                        The sofa is the closest object to the TV, as seen in the keyframe.
                    </div>
                </div>
            </div>
        </div>
    </div>

    <h1>Conclusion</h1>
    <div style="text-align: justify; margin: 0 auto;">
        <p>
            We revisit temporal search for long-form video understanding, focusing on a core challenge for state-of-the-art (SOTA) long-context vision-language models (VLMs). First, we formalize temporal search as a "Long Video Haystack" problem: finding a minimal set of relevant frames (among tens of thousands) given specific queries. To validate this, we introduce <strong>LV-HAYSTACK</strong>, featuring 3,874 human-annotated instances and fine-grained evaluation metrics for keyframe search quality and efficiency. Experimental results on <strong>LV-HAYSTACK</strong> reveal a significant gap in temporal search capabilities under SOTA keyframe selection methods. We further propose a lightweight keyframe searching framework, <em>T*</em>, reframing the costly temporal search into a spatial search task. Extensive experiments show that integrating <em>T*</em> into existing methods significantly boosts SOTA performance. We hope <strong>LV-HAYSTACK</strong> and <em>T*</em> will foster impactful algorithmic advances for efficient long-form video understanding.
        </p>
    </div>

    <h1>Acknowledgement</h1>
    <div style="text-align: justify; margin: 0 auto;">
        <p>This research is supported in part by the Stanford HAI Hoffman-Yee grant, Stanford Institute for Human-Centered AI (HAI), the NSF grant 2131111, 2211258, and the DARPA MCS program under Cooperative Agreement N66001-19-2-4032. We thank Kai Zhang for his valuable discussions, Medhini Narasimhan and Xiangyu Peng for their help with the website.</p>
    </div>

    <h1>BibTeX</h1>
    <div style="max-width: 600px; margin: 0 auto;">
        <pre style="font-size: 12px; background-color: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; white-space: pre-wrap; word-break: break-all;">

        </pre>
    </div>
    <br>

</body>
<script src="assets/js/full_screen_video.js"></script>
<script src="assets/js/carousel.js"></script>

<!-- Script for dataset viewer -->
<script>
    // Dataset examples
    const datasetExamples = [
        {
            type: "HAYSTACK-EGO4D",
            question: "Measuring from the closest point of each object, which of these objects (table, stool, sofa, stove) is the closest to the TV?",
            options: ["Table", "Stool", "Sofa", "Stove"],
            correctOption: 2, // 0-indexed, so 2 is "Sofa"
            groundTruth: "The sofa is the closest object to the TV, as seen in the keyframe.",
            keyframes: ["assets/img/keyframe1-1.jpg", "assets/img/keyframe1-2.jpg", "assets/img/keyframe1-3.jpg"],
            videoSrc: "assets/videos/example1.mp4"
        },
        {
            type: "HAYSTACK-LVBENCH",
            question: "Where was the white trash can before I raised it?",
            options: ["Kitchen", "Bathroom", "Living room", "Bedroom"],
            correctOption: 1, // "Bathroom"
            groundTruth: "The white trash can was in the bathroom before it was raised.",
            keyframes: ["assets/img/keyframe2-1.jpg", "assets/img/keyframe2-2.jpg"],
            videoSrc: "assets/videos/example2.mp4"
        },
        {
            type: "HAYSTACK-EGO4D",
            question: "Who did I talk to in the living room?",
            options: ["Mother", "Brother", "Friend", "Nobody"],
            correctOption: 2, // "Friend"
            groundTruth: "The person talked to a friend in the living room as shown in the video sequence.",
            keyframes: ["assets/img/keyframe3-1.jpg", "assets/img/keyframe3-2.jpg", "assets/img/keyframe3-3.jpg", "assets/img/keyframe3-4.jpg"],
            videoSrc: "assets/videos/example3.mp4"
        },
        {
            type: "HAYSTACK-EGO4D",
            question: "What color is the book on the coffee table?",
            options: ["Red", "Blue", "Green", "Black"],
            correctOption: 1, // "Blue"
            groundTruth: "The book on the coffee table is blue as shown in the keyframe.",
            keyframes: ["assets/img/keyframe4-1.jpg", "assets/img/keyframe4-2.jpg"],
            videoSrc: "assets/videos/example4.mp4"
        },
        {
            type: "HAYSTACK-LVBENCH",
            question: "What was placed on the kitchen counter after the groceries were unpacked?",
            options: ["Fruit bowl", "Coffee machine", "Blender", "Nothing"],
            correctOption: 0, // "Fruit bowl"
            groundTruth: "A fruit bowl was placed on the kitchen counter after unpacking the groceries.",
            keyframes: ["assets/img/keyframe5-1.jpg", "assets/img/keyframe5-2.jpg", "assets/img/keyframe5-3.jpg"],
            videoSrc: "assets/videos/example5.mp4"
        },
        {
            type: "HAYSTACK-EGO4D",
            question: "Which appliance was used first in the morning routine?",
            options: ["Coffee maker", "Toaster", "Microwave", "Kettle"],
            correctOption: 3, // "Kettle"
            groundTruth: "The kettle was the first appliance used in the morning routine.",
            keyframes: ["assets/img/keyframe6-1.jpg", "assets/img/keyframe6-2.jpg"],
            videoSrc: "assets/videos/example6.mp4"
        },
        {
            type: "HAYSTACK-LVBENCH",
            question: "What was moved from the living room to the bedroom?",
            options: ["Pillow", "Laptop", "Book", "Plant"],
            correctOption: 1, // "Laptop"
            groundTruth: "The laptop was moved from the living room to the bedroom.",
            keyframes: ["assets/img/keyframe7-1.jpg", "assets/img/keyframe7-2.jpg", "assets/img/keyframe7-3.jpg"],
            videoSrc: "assets/videos/example7.mp4"
        },
        {
            type: "HAYSTACK-EGO4D",
            question: "What color was the car that passed by the window?",
            options: ["Red", "Blue", "Silver", "White"],
            correctOption: 2, // "Silver"
            groundTruth: "A silver car passed by the window as seen in the keyframe.",
            keyframes: ["assets/img/keyframe8-1.jpg", "assets/img/keyframe8-2.jpg"],
            videoSrc: "assets/videos/example8.mp4"
        },
        {
            type: "HAYSTACK-LVBENCH",
            question: "How many people sat at the dining table during dinner?",
            options: ["Two", "Three", "Four", "Five"],
            correctOption: 2, // "Four"
            groundTruth: "Four people sat at the dining table during dinner.",
            keyframes: ["assets/img/keyframe9-1.jpg", "assets/img/keyframe9-2.jpg"],
            videoSrc: "assets/videos/example9.mp4"
        },
        {
            type: "HAYSTACK-EGO4D",
            question: "What was placed in the refrigerator first?",
            options: ["Milk", "Vegetables", "Leftovers", "Juice"],
            correctOption: 0, // "Milk"
            groundTruth: "Milk was the first item placed in the refrigerator.",
            keyframes: ["assets/img/keyframe10-1.jpg", "assets/img/keyframe10-2.jpg", "assets/img/keyframe10-3.jpg"],
            videoSrc: "assets/videos/example10.mp4"
        },
        {
            type: "HAYSTACK-LVBENCH",
            question: "What was hanging on the wall behind the sofa?",
            options: ["Clock", "Painting", "Mirror", "Calendar"],
            correctOption: 1, // "Painting"
            groundTruth: "A painting was hanging on the wall behind the sofa.",
            keyframes: ["assets/img/keyframe11-1.jpg", "assets/img/keyframe11-2.jpg"],
            videoSrc: "assets/videos/example11.mp4"
        }
    ];
    
    let currentExampleIndex = 0;
    let currentKeyframeIndex = 0;
    
    // DOM elements
    const prevButton = document.getElementById('prevExample');
    const nextButton = document.getElementById('nextExample');
    const exampleCounter = document.getElementById('exampleCounter');
    const datasetTypeLabel = document.getElementById('datasetType');
    const questionText = document.getElementById('currentQuestion');
    const optionsContainer = document.getElementById('optionsContainer');
    const revealButton = document.getElementById('revealButton');
    const answerContainer = document.getElementById('answerContainer');
    const groundTruthElement = document.getElementById('groundTruth');
    const videoElement = document.getElementById('previewVideo');
    
    // Load example data
    function loadExample(index) {
        const example = datasetExamples[index];
        
        // Update dataset type
        datasetTypeLabel.textContent = example.type;
        
        // Update question
        questionText.textContent = example.question;
        
        // Update options
        const optionLabels = document.querySelectorAll('.option-label');
        const optionInputs = document.querySelectorAll('.option-radio');
        
        optionLabels.forEach((label, i) => {
            label.textContent = `${String.fromCharCode(65 + i)}. ${example.options[i]}`;
        });
        
        optionInputs.forEach((input, i) => {
            input.checked = (i === example.correctOption);
        });
        
        // Update answers
        groundTruthElement.textContent = example.groundTruth;
        
        // Hide answer container
        answerContainer.style.display = 'none';
        
        // Update counter
        exampleCounter.textContent = `Example ${index + 1} of ${datasetExamples.length}`;
        
        // Update navigation buttons
        prevButton.disabled = (index === 0);
        nextButton.disabled = (index === datasetExamples.length - 1);
        
        // Update video
        videoElement.src = example.videoSrc;
        videoElement.poster = `assets/img/video_preview${index + 1}.jpg`;
        videoElement.load(); // Reload the video with new source
        
        // Update keyframe carousel
        updateKeyframeCarousel(example.keyframes);
        
        // Update button text
        revealButton.innerHTML = '<img src="assets/img/icon_1.png" alt="icon" width="24" height="24">Click to view Human-anotated Keyframes and Ground-truth Answer!';
    }
    
    // Update keyframe carousel
    function updateKeyframeCarousel(keyframes) {
        // Reset current keyframe index
        currentKeyframeIndex = 0;
        
        // Get the slider container
        const sliderContainer = document.querySelector('.keyframe-slider');
        const dotsContainer = document.querySelector('.keyframe-dots');
        
        // Clear existing content
        sliderContainer.innerHTML = '';
        dotsContainer.innerHTML = '';
        
        // Add new keyframes
        keyframes.forEach((src, index) => {
            // Create slide
            const slide = document.createElement('img');
            slide.className = 'keyframe-slide';
            slide.src = src;
            slide.alt = `Key Frame ${index + 1}`;
            slide.style.minWidth = '100%';
            slide.style.height = 'auto';
            slide.style.objectFit = 'contain';
            sliderContainer.appendChild(slide);
            
            // Create dot
            const dot = document.createElement('span');
            dot.className = index === 0 ? 'keyframe-dot active' : 'keyframe-dot';
            dot.style.height = '10px';
            dot.style.width = '10px';
            dot.style.backgroundColor = index === 0 ? '#4E2A84' : '#bbb';
            dot.style.borderRadius = '50%';
            dot.style.display = 'inline-block';
            dot.style.margin = '0 5px';
            dot.style.cursor = 'pointer';
            dot.onclick = function() { showKeyframe(index); };
            dotsContainer.appendChild(dot);
        });
        
        // Hide navigation buttons if only one keyframe
        const prevBtn = document.querySelector('.keyframe-prev');
        const nextBtn = document.querySelector('.keyframe-next');
        const dotsDiv = document.querySelector('.keyframe-dots');
        
        if (keyframes.length <= 1) {
            prevBtn.style.display = 'none';
            nextBtn.style.display = 'none';
            dotsDiv.style.display = 'none';
        } else {
            prevBtn.style.display = 'block';
            nextBtn.style.display = 'block';
            dotsDiv.style.display = 'flex';
        }
    }
    
    // Show specific keyframe
    function showKeyframe(index) {
        const slider = document.querySelector('.keyframe-slider');
        const dots = document.querySelectorAll('.keyframe-dot');
        const slides = document.querySelectorAll('.keyframe-slide');
        
        // Update current index
        currentKeyframeIndex = index;
        
        // Update slider position
        slider.scrollLeft = slides[index].offsetLeft;
        
        // Update dots
        dots.forEach((dot, i) => {
            dot.style.backgroundColor = i === index ? '#4E2A84' : '#bbb';
            dot.className = i === index ? 'keyframe-dot active' : 'keyframe-dot';
        });
    }
    
    // Move to next/previous keyframe
    function nextKeyframe() {
        const slides = document.querySelectorAll('.keyframe-slide');
        if (currentKeyframeIndex < slides.length - 1) {
            showKeyframe(currentKeyframeIndex + 1);
        }
    }
    
    function prevKeyframe() {
        if (currentKeyframeIndex > 0) {
            showKeyframe(currentKeyframeIndex - 1);
        }
    }
    
    // Add event listeners
    prevButton.addEventListener('click', () => {
        if (currentExampleIndex > 0) {
            currentExampleIndex--;
            loadExample(currentExampleIndex);
        }
    });
    
    nextButton.addEventListener('click', () => {
        if (currentExampleIndex < datasetExamples.length - 1) {
            currentExampleIndex++;
            loadExample(currentExampleIndex);
        }
    });
    
    revealButton.addEventListener('click', () => {
        if (answerContainer.style.display === 'none' || answerContainer.style.display === '') {
            answerContainer.style.display = 'block';
            revealButton.textContent = 'Hide Answer';
        } else {
            answerContainer.style.display = 'none';
            revealButton.innerHTML = '<img src="assets/img/icon_1.png" alt="icon" width="24" height="24">Click to view Human-anotated Keyframes and Ground-truth Answer!';
        }
    });
    
    // Add keyframe navigation event listeners
    document.querySelector('.keyframe-next').addEventListener('click', nextKeyframe);
    document.querySelector('.keyframe-prev').addEventListener('click', prevKeyframe);
    
    // Initialize the first example
    loadExample(currentExampleIndex);
</script>

<!-- Carousel script -->
<script>
let slideIndex = 1;
showSlides(slideIndex);

function plusSlides(n) {
  showSlides(slideIndex += n);
}

function currentSlide(n) {
  showSlides(slideIndex = n);
}

function showSlides(n) {
  let i;
  let slides = document.getElementsByClassName("mySlides");
  let dots = document.getElementsByClassName("dot");
  if (n > slides.length) {slideIndex = 1}    
  if (n < 1) {slideIndex = slides.length}
  for (i = 0; i < slides.length; i++) {
    slides[i].style.display = "none";  
  }
  for (i = 0; i < dots.length; i++) {
    dots[i].className = dots[i].className.replace(" active", "");
  }
  slides[slideIndex-1].style.display = "block";  
  dots[slideIndex-1].className += " active";
}
</script>
</html>